\section{Overview} \label{sec:overview}

This section introduces \name and its support for intersection types,
parametric polymorphism and the merge operator. 
It then discusses the issue of coherence and shows how the notion of disjoint
intersection types and disjoint quantification achieves a coherent semantics.
This section uses some syntactic sugar, as well as standard
programming language features, to illustrate the various concepts in \name. 
Although the minimal core language that we formalize in
Section~\ref{sec:fi} does not present all such features and syntactic sugar, 
these are trivial to add.

\subsection{Intersection Types and the Merge Operator}

\paragraph{Intersection types.}
The intersection of type $A$ and $B$ (denoted by \lstinline{A & B} in
\name) contains exactly those values
which can be used as both values of type $A$ and of type $B$. 
For instance, consider the following program in \name:

\begin{lstlisting}
let x : Int & Bool = (*$ \ldots $*) in -- definition omitted
let succ (y : Int) : Int = y+1 in
let not (y : Bool) : Bool = if y then False else True in (succ x, not x)
\end{lstlisting}

\noindent If a value \lstinline{x} has type \lstinline{Int & Bool} then
\lstinline{x} can be used anywhere where either a value of type \lstinline{Int} or  
a value of type \lstinline{Bool} is expected. 
This means that, in the program above
the functions \lstinline{succ} and \lstinline{not} -- simple functions 
on integers and booleans, respectively -- both accept \lstinline{x} as an argument. 

\paragraph{Merge operator.}
The previous program deliberately omitted the introduction of values of an
intersection type. 
There are many variants of intersection types in the literature. 
Our work follows a particular
formulation, where
intersection types are introduced by a \emph{merge operator}~\cite{reynolds1997design,reynolds1991coherence,castagna1995calculus,dunfield2014elaborating,oliveira16disjoint}. 
As Dunfield~\cite{dunfield2014elaborating} has argued a merge operator adds considerable
expressiveness to a calculus. 
The merge operator allows two values to be merged in a single intersection type. 
For example, an implementation of \lstinline{x} in \name is \lstinline{1,,True}.
Following Dunfield's notation the merge of $v_1$ and $v_2$ is denoted by $v_1 ,, v_2$.

\begin{comment}
\paragraph{Merge vs Pairs}
The significant difference between intersection types with a
merge operator and regular pairs is in the elimination construct. 
With pairs there are explicit eliminators (\lstinline{fst} and
\lstinline{snd}), and these eliminators must be used to extract the
components of the right type.
With intersection types and a merge operator, eliminators are implicit in the language,
meaning no uses of projection functions are necessary.
\end{comment}

%\paragraph{Merge operator and pairs.}
%The merge operator is similar to the introduction construct on pairs.
%An analogous implementation of \lstinline{x} with pairs would be:

%\begin{lstlisting}
%let xPair : (Int, Char) = (1, 'c') in (*$ \ldots $*)
%\end{lstlisting}

% For example, in order to use
%\lstinline{idInt} and \lstinline{idChar} with pairs, we would need to
%write a program such as:

%\begin{lstlisting}
%(idInt (fst xPair), idChar (snd xPair))
%\end{lstlisting}

%\noindent In contrast the elimination of intersection types is done
%implicitly, by following a type-directed process. For example,
%when a value of type \lstinline{Int} is needed, but an intersection
%of type \lstinline{Int & Char} is found, the compiler uses the
%type system to extract the corresponding value.

\subsection{Coherence and Disjointness}
\label{subsec:coherence}
Coherence is a desirable property for a semantics. 
A semantics is coherent if any \emph{valid program} has exactly one
meaning~\cite{reynolds1991coherence} (that is, the semantics is not ambiguous).
Unfortunately the implicit nature of elimination for intersection
types built with a merge operator can lead to incoherence.
This is due to intersections with overlapping types, as in
$\tyint \inter \tyint$.
The result of the program (\lstinline$(1,,2) : Int$)
can be either \lstinline$1$ or \lstinline$2$, depending on the implementation 
of the language.

\paragraph{Disjoint intersection types}
One option to restore coherence is to reject programs which may have
multiple meanings.  The \oldname calculus~\cite{oliveira16disjoint} --
a simply-typed calculus with intersection types and a merge operator
-- solves this problem by using the concept of disjoint intersections.
The incoherence problem with the expression $1 \mergeop 2$
happens because there are two overlapping integers in the merge. 
Generally speaking, if both terms can be assigned some type $C$
then both of them can be chosen as the meaning of the merge,
which in its turn leads to multiple meanings of a term.
Thus a natural option is to forbid such overlapping
values of the same type in a merge. In \oldname 
intersections such as \lstinline{Int&Int} are forbidden, since 
the types in the intersection overlap (i.e. they are not disjoint). However an intersection 
such as \lstinline{Char&Int} is OK because the set of characters 
and integers are disjoint to each other. 

\begin{comment}
This is precisely the approach taken in \oldname: a merge can only be composed of
two values as long as their types are \emph{disjoint}.  
Disjointness is a binary relation between two types, defined for any types which
do not contain any overlapping types. For example \lstinline{Int&Char}
is disjoint to \lstinline{Bool} because  but \lstinline{
\end{comment}

%It can be specified as: given any two types, they are disjoint if there does not 
%exist any common super-type.

\begin{comment}
More formally, the notion of disjointness can be specified as follows:
\bruno{I think we should avoid presenting the specification, since we
  do not have one for this paper. We can refer to this in the related
  work.}

\begin{definition}[Disjointness]
  Given two types $A$ and $B$, two types are disjoint
  (written $A \disjoint B$) if there is no type $C$ such that both $A$ and $B$ are
  subtypes of $C$:
  \[A \disjoint B \equiv \not\exists C.~A \subtype C \wedge B \subtype C\]
\end{definition}

With this concept of disjointness in mind, it is easy to verify that the previous example 
will no longer be accepted since \lstinline$((1,,2) : Int)$ is no longer well-typed!
The merge operator requires two types to be disjoint in order to type-check it into
an intersection.
In the example \lstinline$(1,,2)$ is rejected by the compiler, since \lstinline$Int$ is not
disjoint with \lstinline$Int$.
In other words, there exists a super-type of both \lstinline$Int$ and \lstinline$Int$, 
namely \lstinline$Int$ itself.
This result can be generalized and \oldname has shown that it can lead to a coherent calculus. 
Although this is a promising result, the question remains: is it possible to incorporate 
parametric polymorphism in such calculus, while retaining coherence?
\end{comment}

%\oldname not only provided a specification for disjointness but also an algorithmic version
%of it, and proved that both versions are equivalent.
%This turned disjointness as a concept which is both easy to understand and easy to implement. 

\begin{comment}
\subsection{Top-like types}\bruno{You are spending too much time to
  get to the point. When writing a paper you want to get to the point
  (what's the problem) 
as fast as possible. So you should have enough background to understand
the paper, but keep that background minimal. Look at the ICFP paper: 
we got to Section 2.2 (the problem) in less than a column.
I think we don't need to
cover top-like types here. They are not essential. }
The \oldname calculus also showed how to extend the type system with a type $\top$, the supertype 
of all types.
Since introducing $\top$ leads to a useless disjointness specification (i.e. no type is disjoint to
any other type) and introduces some ambiguity because $\top \subtype \top \inter \top$ and
$\top \inter \top \subtype \top$.
Therefore, the specification was changed to the following:

\begin{definition}[$\top$-disjointness]
  Two types $A$ and $B$ are disjoint
  (written $A \disjoint B$) if the following two conditions are satisfied:
\begin{enumerate}
  \item $(\text{not}~\toplike{A})~\text{and}~(\text{not}~\toplike{B}) $
  \item $\forall C.~\text{if}~ A \subtype C~\text{and}~B \subtype C~\text{then}~\toplike{C}$
\end{enumerate}
\end{definition}
The unary relation $\toplike{\cdot}$ represents the so-called top-like types, which are types that resemble 
$\top$.
This set of types includes $\top$ itself, intersections composed of other 
top-like types (i.e. $\top \inter \top$), and pre-top-types, which are functions that have
$\top$ as their co-domain (i.e. $\tyint \to \tychar \to \top$).
\end{comment}

\subsection{Parametric Polymorphism}
Unfortunately, combining parametric polymorphism with disjoint
intersection types is non-trivial. Consider the following program
(uppercase Latin letters denote type variables):

\begin{lstlisting}
let merge3 A (x : A) : A & Int = x,,3 in
\end{lstlisting}

\noindent The \lstinline{merge3} function takes an argument
\lstinline{x} of some type (\lstinline{A}) and merges \lstinline{x}
with \lstinline{3}. Thus the return type of the program is
\lstinline{A & Int}. \lstinline{merge3} is unproblematic for many
possible instantiations of \lstinline{A}.  However, if
\lstinline{merge3} instantiates \lstinline{A} with a type that
overlaps (i.e. is not disjoint) with \lstinline{Int}, then incoherence may happen.
For example:

\begin{lstlisting}
merge3 Int 2
\end{lstlisting}

\noindent can evaluate to both 2 or 3. 

\paragraph{Forbidding type variables in intersections} 
A naive way to ensure that only programs with disjoint types are accepted is 
simply to forbid type variables in intersections. That is, an
intersection type such as \lstinline{Char&Int} would be accepted, 
but an intersection such as \lstinline{A&Int} (where \lstinline{A} is
some type variable) would be rejected. The reasoning behind this
design is that type variables can be instantiated to any types,
including those already in the intersection. Thus forbidding type
variables in the intersection will prevent invalid intersections
arising from instantiations with overlapping types.
Such design does guarantee coherence and would prevent
\lstinline{merge3} from type-checking. Unfortunately the big drawback
is that the design is too conservative and many other (useful) programs would be 
rejected. In particular, the \lstinline{extend} function from Section~\ref{sec:intro}
would also be rejected. 

\paragraph{Other approaches}
Another option to mitigate the issues of incoherence, without the use of 
disjoint intersection types, is to allow for a biased choice: 
multiple values of the same type may exist in an intersection, but an
implementation gives preference to one of them. The encodings of merge 
operators in TypeScript and Scala~\cite{oliveira2013feature,Rendel14OA} use such an approach.
A first problem with this approach, which has already been pointed out by
Dunfield~\cite{dunfield2014elaborating}, is that the choice of the corresponding value is
tied up to a particular choice in the implementation. In other words
incoherence still exists at the semantic level, but the implementation makes it predictable 
which overlapping value will be chosen. From the
theoretical point-of-view it would be much better to have a clear,
coherent semantics, which is independent from concrete implementations.
Another problem is that the interaction between 
biased choice and polymorphism can lead to counter-intuitive programs, since
instantiation of type-variables affects the type-directed lookup of a
value in an intersection.

\begin{comment}
\paragraph{Biased choice} An option to mitigate the issues of
incoherence, without using disjoint intersection types
is to allow for a biased
choice: that is multiple values of the same type may exist in an
intersection, but an implementation gives preference to one of them.

A first problem with this approach, which has already been pointed out by
Dunfield~\cite{}, is that the choice of the corresponding value is
tied up to a particular choice in the implementation. From the
theoretical point-of-view it would be much better to have a clear, and
unambiguous semantics. 

However, things get even worse in the  presence of polymorphism.
Consider the attempt to write the following polymorphic function in such system: 
\begin{lstlisting}
let fst A B (x: A & B) : A = x in (*$ \ldots $*)
\end{lstlisting}
The \code{fst} function is supposed to extract a value of type
\lstinline{A} from the merge value $x$ (of type \lstinline{A&B}). 
This function can be problematic when
\lstinline{A} and \lstinline{B} are instantiated to non-disjoint
types. 

\paragraph{Biased choice breaks equational reasoning.} 
At first sight, one option
to workaround the incoherence issue would be to bias the type-based merge lookup
to the left or to the right. %(as discussed in Section~\ref{subsec:incoherence}). 
However, biased choice is
very problematic when parametric polymorphism is present in the language.
To see the issue, suppose we chose to always pick the
rightmost value in a merge when multiple values of same type exist.
Intuitively, it would appear that the result of the use of
\lstinline{fst} above is $2$. 
Indeed simple equational reasoning seems to validate such result:
\begin{lstlisting}
   fst Int Int (1,,2)
(*$ \rightsquigarrow $*) ((fun z (*$ \to $*) z) : Int (*$ \to $*) Int) (1,,2) -- (* \textnormal{By the definition of \code{fst}} *)
(*$ \rightsquigarrow $*) ((fun z (*$ \to $*) z) : Int (*$ \to $*) Int) 2      -- (* \textnormal{Right-biased coercion} *)
(*$ \rightsquigarrow $*) 2                                  -- (* \textnormal{By $\beta$-reduction} *)
\end{lstlisting}

However (assuming a straightforward implementation of right-biased
choice) the result of the program would be 1! The reason for this has
todo with \emph{when} the type-based lookup on the merge happens. In
the case of \lstinline{fst}, lookup is triggered by a coercion
function inserted in the definition of \lstinline{fst} at
compile-time.
In the definition of \lstinline$fst$ all it is known is that a
value of type $A$ should be returned from a merge with an intersection
type $A\&B$.  
Clearly the only type-safe choice to coerce the value of type $A\&B$ into $A$ is to
take the left component of the merge. 
This works perfectly for merges
such as \lstinline$(1,,'c')$, where the types of the first and second components
of the merge are disjoint. 
For the merge \lstinline$(1,,'c')$, if a integer lookup
is needed, then \lstinline$1$ is the rightmost integer, which is consistent with the
biased choice. 
Unfortunately, when given the merge \lstinline$(1,,2)$ the
left-component \lstinline$1$ is also picked up, even though in this case \lstinline$2$
is the rightmost integer in the merge. 

The subtle interaction of polymorphism and type-based lookup
means that equational reasoning is broken.
In the equational reasoning steps above, doing apparently correct
substitutions lead us to a wrong result. 
This is a major problem for biased choice and a reason to dismiss it as a possible implementation
choice for \name.

\paragraph{A more conservative attempt.}
Another attempt at restoring coherence can be to forbid type variables inside
intersections (i.e. type variables are not disjoint to any type).
This conservative approach would solve the problem of coherence, but it would also greatly 
restrict the expressiveness of the resulting language.
For example, the function $fst$ defined above, would no longer be accepted by the system.
In fact, parametric polymorphism and intersection types could only be mixed in a very limited manner -
as long as variables do not reside under intersections - and this is arguably a useful improvement
in respect to other standard type systems, such as System $F$.
\end{comment}


\begin{comment}
\subsection{Disjoint Polymorphism}
A more liberal solution,which enables the combination of type
variables and intersection types, is disjoint polymorphism.
Disjoint polymorphism assign constraints to each type variable, which
allows delaying the check for disjointness until type application/instantiation. 
\end{comment}

\subsection{Disjoint Polymorphism}\label{subsec:disjoint-quantification}
To avoid being overly conservative, while still retaining coherence in the
presence of parametric polymorphism and intersection types, \name uses
\emph{disjoint polymorphism}.
Inspired by bounded quantification~\cite{Cardelli:1994},
where a type variable is constrained by a type bound, disjoint polymorphism 
allows type variables to be constrained so that they are disjoint to some
given types. 

With disjoint quantification a variant of the program $merge3$, which
is accepted by \name, is written as:

\begin{lstlisting}
let merge3 ((*$ \highlight {A *~Int} $*)) (x : A) : A & Int = x,,3 in
\end{lstlisting}

\noindent In this variant the type \lstinline{A} can be instantiated
to any types disjoint to \lstinline{Int}. Such restriction is
expressed by the notation \lstinline{A * Int}, where the left-side of 
\lstinline{*} denotes the type variable being declared (\lstinline{A}), and the
right-side denotes the disjointness constraint (\lstinline{Int}).
For example, 

\begin{lstlisting}
merge3 Bool True
\end{lstlisting}

\noindent is accepted. However, instantiating \lstinline{A} with
\lstinline{Int} fails to type-check. 

\paragraph{Multiple constraints} Disjoint quantification allows 
multiple constraints. For example, the following variant of 
\lstinline{merge3} has an additional boolean in the merge:

\begin{lstlisting}
let merge3b ((*$ \highlight {A *~Int \&~Bool} $*)) (x : A) : A & Int & Bool = x,,3,,True in
\end{lstlisting}

\noindent Here the type variable \lstinline{A} needs to be
disjoint to both \lstinline{Int} and \lstinline{Bool}. In \name
such constraint is specified using an intersection type
\lstinline{Int & Bool}. In general, multiple constraints are specified 
with an intersection of all required constraints. 

\paragraph{Type variable constraints}
Disjoint quantification also allows type variables to be disjoint 
to previously defined type variables. For example, the following
program is accepted by \name:
\begin{lstlisting}
let fst A ((*$ \highlight {B *~A} $*)) (x: A & B) : A = x in (*$ \ldots $*)
\end{lstlisting}
The program has two type variables \lstinline{A} and \lstinline{B}. 
\lstinline{A} is unconstrained and can be instantiated with any type. 
However, the type variable \lstinline{B} can only be instantiated
with types that are disjoint to \lstinline{A}. 
The constraint on \lstinline{B} ensures that the
intersection type \lstinline{A & B} is disjoint for all valid instantiations of $A$ and $B$.
In other words, only coherent uses of \lstinline$fst$ will be accepted.
For example, the following use of \lstinline$fst$:
\begin{lstlisting}
fst Int Char (1,,'c')
\end{lstlisting}
is accepted since \lstinline$Int$ and \lstinline$Char$ are disjoint, thus satisfying the constraint
on the second type parameter of \lstinline$fst$.
Furthermore, problematic uses of \lstinline$fst$, such as:
\begin{lstlisting}
fst Int Int (1,,2)
\end{lstlisting}
\noindent are rejected because \lstinline$Int$ is not disjoint with \lstinline$Int$, thus failing to satisfy the
disjointness constraint on the second type parameter of \lstinline$fst$.

\paragraph{Empty constraint}
%Even though disjoint quantification solves the problem of coherence, there is still one detail 
%that needs further justification.
The type variable $A$ in the \lstinline$fst$ function has no constraint.
In \name this actually means that $A$ should be associated with the empty constraint,
which raises the question: which type should be used to represent such empty constraint?
Or, in other words, which type is disjoint to every other type? 
It is obvious that this type should be one of the bounds of the subtyping lattice: either $\bot$ or
$\top$.
The essential intuition here is that the more specific a type in the subtyping relation is, the less types
exist that are disjoint to it.
For example, $\tyint$ is disjoint to all types except the n-ary intersections that contain $\tyint$, 
and $\bot$; while $\tyint \inter \tychar$ is disjoint to all types that do not contain $\tyint$ or
$\tychar$, and $\bot$.
%Thus, the more specific a type variable constraint is, the less options we have to instantiate it with.
This reasoning implies that $\top$ should be treated as the empty constraint.
Indeed, in \name, a single type variable $A$ is only syntactic sugar
for $A * \top$.

%Let us assume two distinct interpretations for the subtyping relation.
%Given two types $A$ and $B$, we can say relation is either:
%\begin{enumerate}
%\item Subset relation ($A \subseteq B$): every element of type $A$ is also of type $B$; or
%\item Coercion ($\exists t. t : A \to B$): every element of type $A$ can be coerced into type $B$.
%\end{enumerate}

\begin{comment}
First, if we consider our subtyping lattice as unbounded (i.e. no $\top$ and no $\bot$), then we have that
disjointness is covariant with respect to the subtyping relation.
More formally:
\[ \inferrule {\jwf \Gamma {A \disjoint B} \\ B \subtype C }
              {\jwf \Gamma {A \disjoint C}} \]

To illustrate this, take $A$ as $\tyint$ and $B$ as $\tybool \inter \tychar$.
This lemma states that every supertype of $\tybool \inter \tychar$, namely $\tybool$, $\tychar$ and 
$\tybool \inter \tychar$ itself are also disjoint with $\tyint$.
Coming back to a bounded subtyping lattice, let us now consider both bounds. 
If some type $A$ were to be disjoint with $\bot$, then by the lemma above $A$ will be disjoint to
virtually any type.
This means that, if $A$ is a type variable, then the possible types that it can be instantiated with
are the ones which are disjoint with every other type (otherwise the lemma above will no longer hold).
Clearly $\bot$ is not a suitable option,  
In other words, we can think of $\bot$ as the type as specific as the infinite intersection.
Conversely, $\top$ can be thought as specific - or rather, as general - as the 0-ary intersection.
\end{comment}

%\subsection{Stability of Substitutions}
%From the technical point of view, the main challenge in the design of
%\name is that, \emph{in general, types are not stable under
%substitution}. This contrasts, for example, with System $F$ where types
%are stable under substitution. That is in System $F$ the following
%property (among others) holds: 
%
%\begin{restatable}[Stability of Substitution]{lemma}{subrefl}
%  \label{lemma:subst_stable}
%  For any well-formed types $A$ and $B$, and a type variable $\alpha$, the result of substituting 
%  $\alpha$ for $A$ in $B$ is also a well-formed type.
%\end{restatable}
%
%In \name if a type variable $A$ is substituted in a type $T_1$, for a type $T_2$ 
%(written $\subst {T_2} A {T_1}$), where $T_1$ and $T_2$ are well-formed, the resulting type might be 
%ill-formed. 
%To understand why, recall the previous example: 
%\begin{lstlisting}
%fst Int Int (1,,2)
%\end{lstlisting}
%The type signature of \lstinline$fst$ may be read as $\forall A (B * A) . (A \inter B) \to A$.
%An application to the type $\tyint$ will lead to instantiation of the variable $A$, leading to the type
%$\forall (B * \tyint). (\tyint \inter B) \to \tyint$.
%Now, the second $\tyint$ application is problematic, since instantiating $B$ with $\tyint$ will lead to
%the ill-formed type $(\tyint \inter \tyint) \to \tyint$.
%However, from this example it is easy to see that all types which are not problematic are exactly the
%ones disjoint with $A$.
%This paper shows how a weaker version of the usual type substitution stability still holds, 
%namely by requiring that the type variable's disjointness constraint is compatible
%with the type as target of the instantiation. 
